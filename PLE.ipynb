{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gc\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.layers import Layer\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "from tensorflow.keras.models import Model,load_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,ReduceLROnPlateau,EarlyStopping\n",
    "\n",
    "from tensorflow.keras import optimizers,initializers\n",
    "from tensorflow.keras.initializers import glorot_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path='./split_data/encoder.pkl'\n",
    "with open(file_path, 'rb') as f:\n",
    "    encoder=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('./split_data/train.csv')\n",
    "test=pd.read_csv('./split_data/test.csv')\n",
    "val=pd.read_csv('./split_data/val.csv')\n",
    "train_num=len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanPoolLayer(Layer):\n",
    "    def __init__(self, axis, **kwargs):\n",
    "        super(MeanPoolLayer, self).__init__(**kwargs)\n",
    "        self.axis = axis\n",
    "\n",
    "    def call(self, x, mask):\n",
    "        mask = tf.expand_dims(tf.cast(mask,tf.float32),axis = -1)\n",
    "        x = x * mask\n",
    "        return K.sum(x, axis=self.axis) / (K.sum(mask, axis=self.axis) + 1e-9)\n",
    "\n",
    "class PleLayer(tf.keras.layers.Layer):\n",
    "    '''\n",
    "    n_experts:list,每个任务使用几个expert。[2,3]第一个任务使用2个expert，第二个任务使用3个expert。\n",
    "    n_expert_share:int,共享的部分设置的expert个数。\n",
    "    expert_dim:int,每个专家网络输出的向量维度。\n",
    "    n_task:int,任务个数。\n",
    "    '''\n",
    "    def __init__(self,n_task,n_experts,expert_dim,n_expert_share,dnn_reg_l2 = 1e-5):\n",
    "        super(PleLayer, self).__init__()\n",
    "        self.n_task = n_task\n",
    "        \n",
    "        # 生成多个任务特定网络和1个共享网络。\n",
    "        self.E_layer = []\n",
    "        for i in range(n_task):\n",
    "            sub_exp = [Dense(expert_dim,activation = 'relu') for j in range(n_experts[i])]\n",
    "            self.E_layer.append(sub_exp)\n",
    "            \n",
    "        self.share_layer = [Dense(expert_dim,activation = 'relu') for j in range(n_expert_share)]\n",
    "        #定义门控网络\n",
    "        self.gate_layers = [Dense(n_expert_share+n_experts[i],kernel_regularizer=regularizers.l2(dnn_reg_l2),\n",
    "                                  activation = 'softmax') for i in range(n_task)]\n",
    "\n",
    "    def call(self,x):\n",
    "        #特定网络和共享网络\n",
    "        E_net = [[expert(x) for expert in sub_expert] for sub_expert in self.E_layer]\n",
    "        share_net = [expert(x) for expert in self.share_layer]\n",
    "        \n",
    "        #门的权重乘上，指定任务和共享任务的输出。\n",
    "        towers = []\n",
    "        for i in range(self.n_task):\n",
    "            g = self.gate_layers[i](x)\n",
    "            g = tf.expand_dims(g,axis = -1) #(bs,n_expert_share+n_experts[i],1)\n",
    "            _e = share_net+E_net[i]  \n",
    "            _e = Concatenate(axis = 1)([expert[:,tf.newaxis,:] for expert in _e]) #(bs,n_expert_share+n_experts[i],expert_dim)\n",
    "            _tower = tf.matmul(_e, g,transpose_a=True)\n",
    "            towers.append(Flatten()(_tower)) #(bs,expert_dim)\n",
    "        return towers\n",
    "\n",
    "def build_ple(sparse_cols,dense_cols,sparse_max_len,embed_dim,expert_dim = 4,\n",
    "              varlens_cols = [],varlens_max_len = [],dnn_hidden_units = (64,64),\n",
    "              n_task = 2,n_experts = [2,2],n_expert_share = 4,dnn_reg_l2 = 1e-6,\n",
    "              drop_rate = 0.0,embedding_reg_l2 = 1e-6,targets = []):\n",
    "\n",
    "   #输入部分，分为sparse,varlens,dense部分。\n",
    "    sparse_inputs = {f:Input([1],name = f) for f in sparse_cols}\n",
    "    dense_inputs = {f:Input([1],name = f) for f in dense_cols}\n",
    "    varlens_inputs = {f:Input([None,1],name = f) for f in varlens_cols}\n",
    "        \n",
    "    input_embed = {}\n",
    "    #离散特征，embedding到k维\n",
    "    for f in sparse_cols:\n",
    "        _input = sparse_inputs[f]\n",
    "        embedding = Embedding(sparse_max_len[f], embed_dim, \n",
    "            embeddings_regularizer=tf.keras.regularizers.l2(embedding_reg_l2)) \n",
    "        input_embed[f] =Flatten()(embedding(_input)) #(bs,k)\n",
    "        \n",
    "    #多标签离散变量\n",
    "    for f in varlens_inputs:\n",
    "        _input = varlens_inputs[f]\n",
    "        mask = Masking(mask_value = 0).compute_mask(_input)\n",
    "        embedding = Embedding(varlens_max_len[f], embed_dim,\n",
    "            embeddings_regularizer=tf.keras.regularizers.l2(1e-6))\n",
    "        _embed =Reshape([-1,embed_dim])(embedding(_input))\n",
    "        out_embed = MeanPoolLayer(axis=1)(_embed,mask)\n",
    "        input_embed[f] = out_embed\n",
    "        \n",
    "    input_embed.update(dense_inputs) #加入连续变量\n",
    "    input_embed = Concatenate(axis = -1)([input_embed[f] for f in input_embed])    \n",
    "                                  \n",
    "    for num in dnn_hidden_units:\n",
    "        input_embed = Dropout(drop_rate)(Dense(num,activation = 'relu',\n",
    "                    kernel_regularizer=regularizers.l2(dnn_reg_l2))(input_embed))\n",
    "    #Ple网络层\n",
    "    towers = PleLayer(n_task,n_experts,expert_dim,n_expert_share)(input_embed)\n",
    "    outputs = [Dense(1,activation = 'sigmoid',kernel_regularizer=regularizers.l2(dnn_reg_l2),\n",
    "                       name = f,use_bias = True)(_t) for f,_t in zip(targets,towers)]\n",
    "    inputs = [sparse_inputs[f] for f in sparse_inputs]+[varlens_inputs[f] for f in varlens_inputs]\\\n",
    "                +[dense_inputs[f] for f in dense_inputs]\n",
    "    model = Model(inputs,outputs) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO：这里要对数组进行处理，原本的数组是字符串，这里处理成array，否则会出现报错：ValueError: Invalid dtype: str3520\n",
    "\n",
    "def parse_multiple_str_arrays(df, columns):\n",
    "    for col in columns:\n",
    "        df[col] = df[col].apply(lambda s: list(map(int, s.replace('\\n', ' ').strip('[]').split())))\n",
    "    return df\n",
    "\n",
    "train = parse_multiple_str_arrays(train, ['manual_keyword_list', 'manual_tag_list'])\n",
    "val = parse_multiple_str_arrays(val, ['manual_keyword_list', 'manual_tag_list'])\n",
    "test = parse_multiple_str_arrays(test, ['manual_keyword_list', 'manual_tag_list'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [\"read_comment\", \"like\", \"click_avatar\", \"forward\"]\n",
    "sparse_features = ['userid', 'feedid', 'authorid', 'bgm_song_id', 'bgm_singer_id']\n",
    "varlen_features = ['manual_tag_list','manual_keyword_list']\n",
    "dense_features = ['videoplayseconds']\n",
    "\n",
    "# 生成输入特征设置\n",
    "sparse_max_len = {f:len(encoder[f]) + 1 for f in sparse_features}\n",
    "varlens_max_len = {f:len(encoder[f]) + 1 for f in varlen_features}\n",
    "feature_names = sparse_features+varlen_features+dense_features\n",
    "\n",
    "# 构建输入数据\n",
    "train_model_input = {name: train[name] if name not in varlen_features else np.stack(train[name]) for name in feature_names } #训练模型的输入，字典类型。名称和具体值\n",
    "val_model_input = {name: val[name] if name not in varlen_features else np.stack(val[name]) for name in feature_names }\n",
    "test_model_input = {name: test[name] if name not in varlen_features else np.stack(test[name]) for name in feature_names}\n",
    "\n",
    "train_labels = [train[y].values for y in target]\n",
    "val_labels = [val[y].values for y in target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/keras/src/optimizers/base_optimizer.py:86: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 构建模型，训练和评估\n",
    "model = build_ple(sparse_features,dense_features,sparse_max_len,embed_dim = 16,expert_dim = 32,\n",
    "          varlens_cols = varlen_features,varlens_max_len = varlens_max_len,dnn_hidden_units = (64,),\n",
    "          n_task = 4,n_experts = [4,4,4,4],n_expert_share = 8,dnn_reg_l2 = 1e-6,\n",
    "          drop_rate = 0.1,embedding_reg_l2 = 1e-6,targets = target)\n",
    "\n",
    "adam = optimizers.Adam(learning_rate=0.01, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "model.compile(adam, loss = 'binary_crossentropy' ,metrics = [tf.keras.metrics.AUC()],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "None values not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_model_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_model_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43mval_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10240\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/keras/src/backend/tensorflow/core.py:153\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(x, dtype, sparse, ragged)\u001b[0m\n\u001b[1;32m    151\u001b[0m         x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x)\n\u001b[1;32m    152\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcast(x, dtype)\n\u001b[0;32m--> 153\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m standardize_dtype(x\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;241m==\u001b[39m dtype:\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, tf\u001b[38;5;241m.\u001b[39mSparseTensor):\n",
      "\u001b[0;31mValueError\u001b[0m: None values not supported."
     ]
    }
   ],
   "source": [
    "history = model.fit(train_model_input, train_labels,validation_data = (val_model_input,val_labels),\n",
    "                    batch_size=10240, epochs=4, verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
